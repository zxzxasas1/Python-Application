{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95434492  1.03673227 -0.04773707  0.05665989]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "[0.06592971 0.2994101 ] 0.34040549248996177\n",
      "[np.float64(0.7219881357765339) np.float64(0.7382190026496331)\n",
      " np.float64(0.4880679972525585) np.float64(0.5141611846642938)]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "   micro avg       0.75      0.75      0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n",
      "[[0.20039829 1.00704966]] [-0.53125836]\n",
      "[ 1.48031185  1.84709861 -1.79831998 -1.51978565]\n",
      "[0.81461968 0.86378609 0.1420557  0.17949309]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用 Logistic Regression 模型來區分資料成兩類\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression 特性 : \n",
    "\n",
    "# 1. 本身具有機率的成分，不光是預測是哪個類別，而是預測樣本屬於該類別的 \" 機率 \"\n",
    "\n",
    "# 2. 雖然轉換成機率的 Sigmoid 函數是非線性，但本身模型還是 ax + b 線性，所以算線性模型，比較適合線性資料\n",
    "\n",
    "# 3. 是 ANN 類神經網路裡面一個神經元 , 函數是 Sigmoid 機率函數的特例\n",
    "\n",
    "\n",
    "\n",
    "# a. 下面範例為簡單的四個點，分別在 y 軸的上面和下面，基本上可以用線性的方法來回歸區分出兩類\n",
    "\n",
    "# 1. 確實線性回歸可以做到基本的分類，和 Logistic Regression 一樣\n",
    "\n",
    "# 2. 但是會缺乏機率的結果呈現和解釋，沒有使用 Sigmoid 函數將結果限制在 0 到 1 之間\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "\n",
    "x = numpy.array([[2.5,1.5],[-1.7,2.7],[-1.8,-0.9],[1.6,-1.3]])\n",
    "y = numpy.array([1,1,0,0])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "y_prediction = model.predict(x)\n",
    "\n",
    "print(y_prediction)\n",
    "\n",
    "y_map = []\n",
    "\n",
    "# 要自己做閥值和區分\n",
    "\n",
    "for item in y_prediction : \n",
    "\n",
    "    if item >= 0.5 : \n",
    "\n",
    "        y_map.append(1)\n",
    "\n",
    "    else : \n",
    "\n",
    "        y_map.append(0)\n",
    "\n",
    "report = classification_report(y,y_map,target_names=[\"0\",\"1\"])\n",
    "\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n",
    "# b. 可以發現分類效果還蠻好的 ! 但是 \n",
    "\n",
    "# 1. y_prediction 會超過 0 ~ 1 之間，不像是機率的感覺\n",
    "\n",
    "# 2. 所以要有 Sigmoid 函數的轉換來得到機率，下面做一個用 Sigmoid 函數轉換 y_prediction 的版本\n",
    "\n",
    "\n",
    "\n",
    "x = numpy.array([[2.5,1.5],[-1.7,2.7],[-1.8,-0.9],[1.6,-1.3]])\n",
    "y = numpy.array([1,1,0,0])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "print(model.coef_,model.intercept_)\n",
    "\n",
    "\n",
    "# 這邊做 Sigmoid 函數，然後要做一個函數來處理 sigmoid 函數的代入值為陣列時也傳回陣列的版本\n",
    "\n",
    "# 基本上這邊因為用 numpy.exp() 所以可以處理陣列，但是如果是 math.exp() 就一定要多做一個函數來處理\n",
    "\n",
    "def sigmoid(u) :  \n",
    "    \n",
    "    value = 1 / (1 + numpy.exp(-u))\n",
    "\n",
    "    return value\n",
    "\n",
    "linear_y = model.predict(x)\n",
    "\n",
    "# 使用 frompyfunc 來將函數轉成可以回傳陣列\n",
    "\n",
    "# 後面兩個參數是每次輸入 1 個值 ( u_value ) 就產生 1 個值 ( sigmoid(u) )，最後形成陣列\n",
    "\n",
    "array_sigmoid = numpy.frompyfunc(sigmoid,1,1)\n",
    "\n",
    "# 使用上有 lamda 的成分，上面 frompyfunc 裡面的 sigmoid 不用放參數，放在下面呼叫時\n",
    "\n",
    "\n",
    "y_prediction = array_sigmoid(linear_y)\n",
    "\n",
    "print(y_prediction)\n",
    "\n",
    "y_map_2 = []\n",
    "\n",
    "for item in y_prediction : \n",
    "\n",
    "    if item >= 0.5 : \n",
    "\n",
    "        y_map_2.append(1)\n",
    "\n",
    "    else : \n",
    "\n",
    "        y_map_2.append(0)\n",
    "\n",
    "report_2 = classification_report(y,y_map_2,target_names=[\"0\",\"1\"])\n",
    "\n",
    "print(report_2)\n",
    "\n",
    "\n",
    "\n",
    "# c. 可以發現分類的 y_prediction 確實變成機率\n",
    "\n",
    "# 1. 但是分類的機率會變成有點集中，不太好區分\n",
    "\n",
    "# 2. 因為 Logistic Regression 的機率分布假設和 Linear Regression 不同，因此 Linear Regression 預測值套上 Sigmoid 產生機率會怪怪的\n",
    "\n",
    "# 3. 也因為機率分布假設不同，因此 Logistic Regression 的閥值不會是線性一半一半切分，而是有 log 的非線性特性\n",
    "\n",
    "# 4. Linear Regression 是用 MSE 來求參數，而 Logistic Regression 是用 MLE 來求參數，MLE 才有機率的概念\n",
    "\n",
    "# 5. 下面修正成 Logistic Regression 並且用 Sigmoid 函數看預測值，就可以發現機率就明顯比較分散\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = numpy.array([[2.5,1.5],[-1.7,2.7],[-1.8,-0.9],[1.6,-1.3]])\n",
    "y = numpy.array([1,1,0,0])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "print(model.coef_,model.intercept_)\n",
    "\n",
    "\n",
    "# 這邊把 y_prediction 改成自己算，看看中間 Logistic Regression 產生的值和 Sigmoid 函數轉換得到的機率\n",
    "\n",
    "# ( Logistic Regression 的 model.predict 是預設 0.5 以上就視為 1 ，小於就視為 0，這邊一樣比照 )\n",
    "\n",
    "# 也可以直接用 model.predict_proba 來算\n",
    "\n",
    "y_logistic = x[:,0]*model.coef_[0,0] + x[:,1]*model.coef_[0,1] + model.intercept_\n",
    "\n",
    "print(y_logistic)\n",
    "\n",
    "# 也可以用 scipy.special 的 expit 來直接求 sigmoid 函數值\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "y_sigmoid = expit(y_logistic)\n",
    "\n",
    "print(y_sigmoid)\n",
    "\n",
    "\n",
    "y_map_3 = []\n",
    "\n",
    "for item in y_sigmoid : \n",
    "\n",
    "    if item >= 0.5 : \n",
    "\n",
    "        y_map_3.append(1)\n",
    "\n",
    "    else : \n",
    "\n",
    "        y_map_3.append(0)\n",
    "\n",
    "report_3 = classification_report(y,y_map_3,target_names=[\"0\",\"1\"])\n",
    "\n",
    "print(report_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e0c44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39863055  0.73804755 -2.13352915 -0.8234161 ]\n",
      " [ 0.27913268 -0.44844007 -0.03008612 -0.78112937]\n",
      " [ 0.11949787 -0.28960748  2.16361527  1.60454547]] [  8.7064509    2.75908099 -11.46553189]\n",
      "[[0.02 0.96 0.02 1.  ]\n",
      " [0.29 0.71 0.   1.  ]\n",
      " [0.   0.04 0.96 2.  ]\n",
      " [0.02 0.89 0.09 1.  ]\n",
      " [0.06 0.88 0.06 1.  ]\n",
      " [0.94 0.06 0.   0.  ]\n",
      " [0.   0.02 0.98 2.  ]\n",
      " [0.11 0.86 0.02 1.  ]\n",
      " [0.98 0.02 0.   0.  ]\n",
      " [0.01 0.76 0.23 1.  ]\n",
      " [0.05 0.89 0.07 1.  ]\n",
      " [0.99 0.01 0.   0.  ]\n",
      " [0.01 0.83 0.16 1.  ]\n",
      " [0.   0.62 0.38 1.  ]\n",
      " [0.95 0.05 0.   0.  ]\n",
      " [0.98 0.02 0.   0.  ]\n",
      " [0.   0.27 0.73 2.  ]\n",
      " [0.01 0.84 0.15 1.  ]\n",
      " [0.98 0.02 0.   0.  ]\n",
      " [0.97 0.03 0.   0.  ]\n",
      " [0.   0.17 0.83 2.  ]\n",
      " [0.03 0.89 0.08 1.  ]\n",
      " [0.   0.01 0.99 2.  ]\n",
      " [0.98 0.02 0.   0.  ]\n",
      " [0.   0.2  0.8  2.  ]\n",
      " [0.15 0.84 0.01 1.  ]\n",
      " [0.96 0.04 0.   0.  ]\n",
      " [0.96 0.04 0.   0.  ]\n",
      " [0.04 0.93 0.03 1.  ]\n",
      " [0.   0.01 0.99 2.  ]\n",
      " [0.   0.23 0.77 2.  ]\n",
      " [0.   0.47 0.53 2.  ]\n",
      " [0.   0.1  0.9  2.  ]\n",
      " [0.   0.17 0.83 2.  ]\n",
      " [0.99 0.01 0.   0.  ]\n",
      " [0.97 0.03 0.   0.  ]\n",
      " [0.   0.06 0.94 2.  ]\n",
      " [0.98 0.02 0.   0.  ]\n",
      " [0.96 0.04 0.   0.  ]\n",
      " [0.01 0.85 0.14 1.  ]\n",
      " [0.   0.42 0.58 2.  ]\n",
      " [0.03 0.9  0.07 1.  ]\n",
      " [0.95 0.05 0.   0.  ]\n",
      " [0.   0.16 0.84 2.  ]\n",
      " [0.   0.27 0.73 2.  ]\n",
      " [0.   0.11 0.89 2.  ]\n",
      " [0.   0.03 0.97 2.  ]\n",
      " [0.01 0.77 0.21 1.  ]\n",
      " [0.98 0.02 0.   0.  ]\n",
      " [0.95 0.05 0.   0.  ]\n",
      " [0.02 0.71 0.27 1.  ]\n",
      " [0.01 0.64 0.36 1.  ]\n",
      " [0.   0.27 0.73 2.  ]\n",
      " [0.97 0.03 0.   0.  ]\n",
      " [0.01 0.85 0.14 1.  ]\n",
      " [0.97 0.03 0.   0.  ]\n",
      " [0.01 0.71 0.28 1.  ]\n",
      " [0.96 0.04 0.   0.  ]\n",
      " [0.01 0.89 0.11 1.  ]\n",
      " [0.   0.57 0.43 1.  ]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        20\n",
      "  versicolor       0.95      1.00      0.98        21\n",
      "   virginica       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zxzxa\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 用 Logistic Regression 進行三組以上分類的案例\n",
    "\n",
    "\n",
    "\n",
    "# 基本觀念還是一樣，只是會變成 n 類就有 n 個迴歸式\n",
    "\n",
    "# 不同的迴歸式得到的 Logistic 值再用函數轉成機率，看哪一組機率比較大就是哪一組\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "\n",
    "\n",
    "# 1. 把資料分成訓練集和測試集\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.4)\n",
    "\n",
    "\n",
    "# 2. 進行迴歸\n",
    "\n",
    "# 這邊用的是 softmax 函數 ( multinominal ) ，是針對多類別的 Logistic Regression 的機率函數\n",
    "\n",
    "# solver 可以選擇 lbfgs , 比較適合小資料集\n",
    "\n",
    "# max_iter 則是試行次數，Logistic Regression 也需要類似梯度下降來逼近求解\n",
    "\n",
    "# 基本上不做設定也是會自己做多類別迴歸\n",
    "\n",
    "model = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",max_iter=200)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "print(model.coef_,model.intercept_)\n",
    "\n",
    "\n",
    "# 3. 產生預測值以及預測機率\n",
    "\n",
    "y_prediction = model.predict(x_test)\n",
    "\n",
    "y_prediction_rate = numpy.round(model.predict_proba(x_test),decimals=2)\n",
    "\n",
    "# 將各樣本的各類別預測機率及預測結果列出來\n",
    "\n",
    "y_test_set = numpy.concatenate((y_prediction_rate,y_prediction.reshape(-1,1)),axis=1)\n",
    "\n",
    "print(y_test_set)\n",
    "\n",
    "\n",
    "# 4. 產生指標報告\n",
    "\n",
    "report = classification_report(y_test,y_prediction,target_names=iris[\"target_names\"])\n",
    "\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
